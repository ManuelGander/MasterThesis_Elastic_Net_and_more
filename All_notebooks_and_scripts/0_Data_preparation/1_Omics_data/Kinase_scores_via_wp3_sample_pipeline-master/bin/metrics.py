import os.path
import os
import sys
import json
import warnings
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Union

import scipy.stats
import numpy as np
import pandas as pd

from . import clinical_process
from . import utils

logger = logging.getLogger(__name__)

MEASURE_NAMES = [('rank', 'rank'), ('fc', 'fold_change'), ('z', 'z-score'), ('p', 'p-value')]


def compute_metrics(results_folder: Union[str, Path],
                    patient_regex: str,
                    debug: bool,
                    data_types: List[str]):
    """Computes rank, fold change, z-score and p-value
    Requires that annot_{data_type}.csv is generated by clinical_process.py
    """
    logger.info("Running compute metrics module")
    
    for data_type in data_types:
        if check_measures_computed(results_folder, data_type, MEASURE_NAMES):
            logger.info(f"Skipping compute_metrics {data_type} - already computed")
            continue

        logger.info(f'Reading in clinically annotated {data_type} data')
        annot, annot_ref = clinical_process.read_annotation_files(results_folder, debug, data_type)
        # filter for patient columns
        annot = annot.filter(regex=patient_regex)
        measures = compute_measures(annot, patient_regex)
        save_measures(results_folder, MEASURE_NAMES, measures, data_type)
        if debug:
            annot_ref = annot_ref.filter(regex=rf'({patient_regex})|(^Reporter intensity corrected)')
            measures_ref = compute_measures(annot_ref, patient_regex)
            save_measures(results_folder, MEASURE_NAMES, measures_ref, data_type, with_reference_channels=True)


class Metrics:
    """
    Describe
    """
    def __init__(self, df):
        self.df = df
        self.metrics_df = {}
        self.calc()

    @staticmethod
    def get_rank(df: pd.DataFrame) -> pd.DataFrame:
        """

        """
        logger.info("Calculating ranks")
        df_rank = df.copy()
        df_rank = df_rank.rank(ascending=False, method='average',
                               na_option='keep', axis=1)
        # add new column that for each protein/peptide tells the max rank
        df_rank['max'] = df_rank.notnull().sum(axis=1)
        return df_rank.add_prefix('rank_')

    @staticmethod
    def get_fold_change(df: pd.DataFrame) -> pd.DataFrame:
        """
        Leave-one-out approach (loo)
        """
        logger.info("Calculating fold changes")
        df_fold_change, df2 = df.copy(), df.copy()
        #logger.info("Calculating fold changes")
        #logger.info('\n%s', df_fold_change.columns.to_string(index=False))
        
        for column in df_fold_change:
            logger.info(column)
            loo_median = np.power(10, df2.drop(column, axis=1)).median(1)
            df_fold_change[column] = np.power(10, df_fold_change)[column].divide(loo_median)
        df_fold_change = df_fold_change.add_prefix('fc_')
        return df_fold_change

    @staticmethod
    def get_zscore(df: pd.DataFrame) -> pd.DataFrame:
        """
        Leave-one-out approach (loo)
        """
        logger.info("Calculating z-scores")
        df_z_score = df.copy()
        df_z_score = pd.DataFrame(df_z_score)

        for column in df_z_score:
            loo_df = df.drop(column, axis=1)
            df_z_score[column] = (df_z_score[column] - loo_df.median(1)) / loo_df.std(1)
        df_z_score.columns = df.columns
        df_z_score.index = df.index
        df_z_score = df_z_score.add_prefix('zscore_')
        return df_z_score

    @staticmethod
    def get_pvalues(z_scores: pd.DataFrame) -> pd.DataFrame:
        """

        """
        logger.info("Calculating p-values")
        df_p_values = scipy.stats.norm.sf(abs(z_scores)) * 2
        df_p_values = pd.DataFrame(df_p_values)
        df_p_values.columns = z_scores.columns
        df_p_values.index = z_scores.index
        return df_p_values.add_prefix('pvalue_')

    def calc(self):
        # add metrics
        self.metrics_df['rank'] = Metrics.get_rank(self.df)
        self.metrics_df['fold_change'] = Metrics.get_fold_change(self.df)
        self.metrics_df['z-score'] = Metrics.get_zscore(self.df)
        self.metrics_df['p-value'] = Metrics.get_pvalues(self.metrics_df['z-score'])
        self.metrics_df['occurrence'] = self.df.count(axis=1)
        self.metrics_df['occurrence'].name = 'occurrence'
        self.metrics_df['occurrence'] = self.metrics_df['occurrence'].to_frame()


def get_metrics(df: pd.DataFrame, patient_regex) -> Dict[str, pd.DataFrame]:
    """
    describe
    """
    logger.info('Calculating metrics')
    
    df = utils.keep_only_sample_columns(df, patient_regex)

    # Get metrics
    m = Metrics(df)
    return m.metrics_df


def get_data_type_long(data_type):
    if data_type == 'pp':
        return 'phospho'
    return 'full_proteome'


def check_measures_computed(results_folder: Union[str, Path],
                            data_type: str,
                            measure_names: List[Tuple[str]] = MEASURE_NAMES) -> Dict[str, pd.DataFrame]:
    """
    explain what modalities and measures have to be

    """    
    data_type_long = get_data_type_long(data_type)
    for m, measure in measure_names:
        filename = os.path.join(results_folder, f'{data_type_long}_measures_{m}.tsv')
        if not os.path.exists(filename):
            return False

    return True


def read_measures(results_folder: Union[str, Path],
                  data_type: str,
                  measure_names: List[Tuple[str]] = MEASURE_NAMES,
                  with_reference_channels: bool = False) -> Dict[str, pd.DataFrame]:
    """
    explain what modalities and measures have to be

    """    
    data_type_long = get_data_type_long(data_type)
    index_col = utils.get_index_cols(data_type)
    
    measures = dict()
    for m, measure in measure_names:
        filename = os.path.join(results_folder, f'{data_type_long}_measures_{m}.tsv')
        if not os.path.exists(filename):
            return dict()
        
        logger.info(f'Reading in {filename}')
        measures[measure] = pd.read_csv(filename, sep='\t', index_col=index_col)
    return measures


def compute_measures(df: pd.DataFrame, patient_regex) -> Dict[str, pd.DataFrame]:
    """
    describe and also what data is
    """
    measures = get_metrics(df, patient_regex)  # ~1 minute

    # add clinical baskets to z-scores dataframe
    if 'rtk' in df.columns:
        measures['z-score'] = measures['z-score'].join(
            df.loc[:, ['basket', 'basket_weights', 'other', 'other_weights', 'rtk', 'rtk_weights']])
    else:
        if 'basket_weights' in df.columns:
            measures['z-score'] = measures['z-score'].join(
                df.loc[:, ['basket', 'basket_weights', 'sub_basket', 'sub_basket_weights']])
    return measures


def save_measures(results_folder: Union[str, Path],
                  measure_names: List[Tuple[str]],
                  measures: Dict[str, pd.DataFrame],
                  data_type: str,
                  with_reference_channels: bool = False):
    # save quantification measures
    data_type_long = get_data_type_long(data_type)
    for m, measure in measure_names:
        filename = os.path.join(results_folder, f'{data_type_long}_measures_{m}.tsv')
        if with_reference_channels:
            filename = os.path.join(results_folder, f'{data_type_long}_measures_{m}_ref.tsv')
        measures[measure].to_csv(filename, sep='\t')


if __name__ == '__main__':
    import argparse
    
    from . import config
    
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--config", required=True,
                        help="Absolute path to configuration file.")
    args = parser.parse_args(sys.argv[1:])

    configs = config.load(args.config)
    
    compute_metrics(configs["results_folder"], configs["preprocessing"]["debug"], data_types=configs["data_types"])
